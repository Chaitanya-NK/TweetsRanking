{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97433b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"â‚¬\" : \"euro\", \"4ao\" : \"for adults only\", \"a.m\" : \"before midday\", \"a3\" : \"anytime anywhere anyplace\", \"aamof\" : \"as a matter of fact\", \"acct\" : \"account\", \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\", \"afaict\" : \"as far as i can tell\", \"afaik\" : \"as far as i know\", \"afair\" : \"as far as i remember\", \"afk\" : \"away from keyboard\", \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\", \"apps\" : \"applications\", \"asap\" : \"as soon as possible\", \"asl\" : \"age, sex, location\", \"atk\" : \"at the keyboard\", \"ave.\" : \"avenue\", \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \"b&b\" : \"bed and breakfast\", \"b+b\" : \"bed and breakfast\", \"b.c\" : \"before christ\", \"b2b\" : \"business to business\", \"b2c\" : \"business to customer\", \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\", \"b@u\" : \"back at you\", \"bae\" : \"before anyone else\", \"bak\" : \"back at keyboard\", \"bbbg\" : \"bye bye be good\", \"bbc\" : \"british broadcasting corporation\", \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\", \"bbs\" : \"be back soon\", \"be4\" : \"before\", \"bfn\" : \"bye for now\", \"blvd\" : \"boulevard\", \"bout\" : \"about\", \"brb\" : \"be right back\", \"bros\" : \"brothers\", \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\", \"btw\" : \"by the way\", \"bwl\" : \"bursting with laughter\", \"c/o\" : \"care of\", \"cet\" : \"central european time\", \"cf\" : \"compare\", \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\", \"cu\" : \"see you\", \"cul8r\" : \"see you later\", \"cv\" : \"curriculum vitae\", \"cwot\" : \"complete waste of time\", \"cya\" : \"see you\", \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\", \"dbmib\" : \"do not bother me i am busy\", \"diy\" : \"do it yourself\", \"dm\" : \"direct message\", \"dwh\" : \"during work hours\", \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\", \"eg\" : \"example\", \"embm\" : \"early morning business meeting\", \"encl\" : \"enclosed\", \"encl.\" : \"enclosed\", \"etc\" : \"and so on\", \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\", \"fb\" : \"facebook\", \"fc\" : \"fingers crossed\", \"fig\" : \"figure\", \"fimh\" : \"forever in my heart\",  \"ft.\" : \"feet\", \"ft\" : \"featuring\", \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\", \"fwiw\" : \"for what it is worth\", \"fyi\" : \"for your information\", \"g9\" : \"genius\", \"gahoy\" : \"get a hold of yourself\", \"gal\" : \"get a life\", \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\", \"gg\" : \"good game\", \"gl\" : \"good luck\", \"glhf\" : \"good luck have fun\", \"gmt\" : \"greenwich mean time\", \"gmta\" : \"great minds think alike\", \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\", \"goat\" : \"greatest of all time\", \"goi\" : \"get over it\", \"gps\" : \"global positioning system\", \"gr8\" : \"great\", \"gratz\" : \"congratulations\", \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\", \"hp\" : \"horsepower\", \"hr\" : \"hour\", \"hrh\" : \"his royal highness\", \"ht\" : \"height\", \"ibrb\" : \"i will be right back\", \"ic\" : \"i see\", \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\", \"idc\" : \"i do not care\", \"idgadf\" : \"i do not give a damn fuck\", \"idgaf\" : \"i do not give a fuck\", \"idk\" : \"i do not know\", \"ie\" : \"that is\", \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\", \"IG\" : \"instagram\", \"iirc\" : \"if i remember correctly\", \"ilu\" : \"i love you\", \"ily\" : \"i love you\", \"imho\" : \"in my humble opinion\", \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\", \"iow\" : \"in other words\", \"irl\" : \"in real life\", \"j4f\" : \"just for fun\", \"jic\" : \"just in case\", \"jk\" : \"just kidding\", \"jsyk\" : \"just so you know\", \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\", \"lbs\" : \"pounds\", \"ldr\" : \"long distance relationship\", \"lmao\" : \"laugh my ass off\", \"lmfao\" : \"laugh my fucking ass off\", \"lol\" : \"laughing out loud\", \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\", \"m8\" : \"mate\", \"mf\" : \"motherfucker\", \"mfs\" : \"motherfuckers\", \"mfw\" : \"my face when\", \"mofo\" : \"motherfucker\", \"mph\" : \"miles per hour\", \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\", \"ms\" : \"miss\", \"mte\" : \"my thoughts exactly\", \"nagi\" : \"not a good idea\", \"nbc\" : \"national broadcasting company\", \"nbd\" : \"not big deal\", \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\", \"nhs\" : \"national health service\", \"nrn\" : \"no reply necessary\", \"nsfl\" : \"not safe for life\", \"nsfw\" : \"not safe for work\", \"nth\" : \"nice to have\", \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\", \"oc\" : \"original content\", \"og\" : \"original\", \"ohp\" : \"overhead projector\", \"oic\" : \"oh i see\", \"omdb\" : \"over my dead body\", \"omg\" : \"oh my god\", \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\", \"p.m\" : \"after midday\", \"pm\" : \"prime minister\", \"poc\" : \"people of color\", \"pov\" : \"point of view\", \"pp\" : \"pages\", \"ppl\" : \"people\", \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\", \"pt\" : \"point\", \"ptb\" : \"please text back\", \"pto\" : \"please turn over\", \"qpsa\" : \"what happens\", \"ratchet\" : \"rude\", \"rbtl\" : \"read between the lines\", \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\", \"roflol\" : \"rolling on the floor laughing out loud\", \"rotflmao\" : \"rolling on the floor laughing my ass off\", \"rt\" : \"retweet\", \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\", \"sk8\" : \"skate\", \"smh\" : \"shake my head\", \"sq\" : \"square\", \"srsly\" : \"seriously\",  \"ssdd\" : \"same stuff different day\", \"tbh\" : \"to be honest\", \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\", \"tfw\" : \"that feeling when\", \"thks\" : \"thank you\", \"tho\" : \"though\", \"thx\" : \"thank you\", \"tia\" : \"thanks in advance\", \"til\" : \"today i learned\", \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\", \"tmb\" : \"tweet me back\", \"tntl\" : \"trying not to laugh\", \"ttyl\" : \"talk to you later\", \"u\" : \"you\", \"u2\" : \"you too\", \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\", \"w/\" : \"with\", \"w/o\" : \"without\", \"w8\" : \"wait\", \"wassup\" : \"what is up\", \"wb\" : \"welcome back\", \"wtf\" : \"what the fuck\", \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\", \"wuf\" : \"where are you from\", \"wuzup\" : \"what is up\", \"wywh\" : \"wish you were here\", \"yd\" : \"yard\", \"ygtr\" : \"you got that right\", \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\", \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\t  \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "    \"how's\": \"how does\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"ur\": \"your\", \"n\": \"and\", \"won't\": \"would not\", \"dis\": \"this\", \"brng\": \"bring\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cad17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8101127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Likes</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>stopwords_counts</th>\n",
       "      <th>hashtag_counts</th>\n",
       "      <th>mentions_counts</th>\n",
       "      <th>digits_counts</th>\n",
       "      <th>uppercase_counts</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-08 05:06:05+00:00</td>\n",
       "      <td>KenRaty</td>\n",
       "      <td>wallybigwall joneshospodtx cfidd i love it but...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>33</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-08 05:06:01+00:00</td>\n",
       "      <td>DileepKumarPak</td>\n",
       "      <td>search and rescue operations resumed at the si...</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>18</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-08 05:06:00+00:00</td>\n",
       "      <td>DSWDfo7</td>\n",
       "      <td>our community volunteers in brgy canagahan of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1904</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>28</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-08 05:05:48+00:00</td>\n",
       "      <td>migzu17</td>\n",
       "      <td>yugivee by accident i was originally going to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>416</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>38</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-08 05:05:47+00:00</td>\n",
       "      <td>darrellriffic</td>\n",
       "      <td>hishyar09205113 mdoe99500351 bozturkserkan dor...</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>57</td>\n",
       "      <td>5.280702</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date            User  \\\n",
       "0  2023-02-08 05:06:05+00:00         KenRaty   \n",
       "1  2023-02-08 05:06:01+00:00  DileepKumarPak   \n",
       "2  2023-02-08 05:06:00+00:00         DSWDfo7   \n",
       "3  2023-02-08 05:05:48+00:00         migzu17   \n",
       "4  2023-02-08 05:05:47+00:00   darrellriffic   \n",
       "\n",
       "                                               Tweet  Retweets  Followers  \\\n",
       "0  wallybigwall joneshospodtx cfidd i love it but...         0        106   \n",
       "1  search and rescue operations resumed at the si...         0        280   \n",
       "2  our community volunteers in brgy canagahan of ...         0       1904   \n",
       "3  yugivee by accident i was originally going to ...         0        416   \n",
       "4  hishyar09205113 mdoe99500351 bozturkserkan dor...         0         54   \n",
       "\n",
       "   Likes  char_counts  word_counts  avg_wordlength  stopwords_counts  \\\n",
       "0      0          168           33        5.090909                16   \n",
       "1      0          138           18        7.666667                 4   \n",
       "2      0          188           28        6.714286                 6   \n",
       "3      0          160           38        4.210526                17   \n",
       "4      0          301           57        5.280702                33   \n",
       "\n",
       "   hashtag_counts  mentions_counts  digits_counts  uppercase_counts  Target  \n",
       "0               0                5              3                 1       1  \n",
       "1               5                0              3                 0       1  \n",
       "2               0                1              7                 1       1  \n",
       "3               0                1              5                 4       1  \n",
       "4               0                4              8                 0       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ac7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcounts(x):\n",
    "    length = len(str(x).split())\n",
    "    return length\n",
    "\n",
    "def get_charcounts(x):\n",
    "    s = x.split()\n",
    "    x = ''.join(s)\n",
    "    return len(x)\n",
    "\n",
    "def get_avg_wordlength(x):\n",
    "    count = get_charcounts(x)/get_wordcounts(x)\n",
    "    return count\n",
    "\n",
    "def get_stopwords_counts(x):\n",
    "    l = len([t for t in x.split() if t in stopwords])\n",
    "    return l\n",
    "\n",
    "def get_hashtag_counts(x):\n",
    "    l = len([t for t in x.split() if t.startswith('#')])\n",
    "    return l\n",
    "\n",
    "def get_mentions_counts(x):\n",
    "    l = len([t for t in x.split() if t.startswith('@')])\n",
    "    return l\n",
    "\n",
    "def get_digit_counts(x):\n",
    "    digits = re.findall(r'[0-9,.]+', x)\n",
    "    return len(digits)\n",
    "\n",
    "def get_uppercase_counts(x):\n",
    "    return len([t for t in x.split() if t.isupper()])\n",
    "\n",
    "def cont_exp(x):\n",
    "    if type(x) is str:\n",
    "        for key in abbreviations:\n",
    "            value = abbreviations[key]\n",
    "            raw_text = r'\\b' + key + r'\\b'\n",
    "            x = re.sub(raw_text, value, x)\n",
    "            # print(raw_text,value, x)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def get_emails(x):\n",
    "    emails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', x)\n",
    "    counts = len(emails)\n",
    "\n",
    "    return counts, emails\n",
    "\n",
    "def remove_emails(x):\n",
    "    return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n",
    "\n",
    "def get_urls(x):\n",
    "    urls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)\n",
    "    counts = len(urls)\n",
    "\n",
    "    return counts, urls\n",
    "\n",
    "def remove_urls(x):\n",
    "    return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
    "\n",
    "def remove_rt(x):\n",
    "    return re.sub(r'\\brt\\b', '', x).strip()\n",
    "\n",
    "def remove_special_chars(x):\n",
    "    x = re.sub(r'[^\\w ]+', \"\", x)\n",
    "    x = ' '.join(x.split())\n",
    "    return x\n",
    "\n",
    "def remove_html_tags(x):\n",
    "    return BeautifulSoup(x, 'lxml').get_text().strip()\n",
    "\n",
    "def remove_accented_chars(x):\n",
    "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return x\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return ' '.join([t for t in x.split() if t not in stopwords])\t\n",
    "\n",
    "def make_base(x):\n",
    "    x = str(x)\n",
    "    x_list = []\n",
    "    doc = nlp(x)\n",
    "\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma == '-PRON-' or lemma == 'be':\n",
    "            lemma = token.text\n",
    "\n",
    "        x_list.append(lemma)\n",
    "    return ' '.join(x_list)\n",
    "\n",
    "def get_value_counts(df, col):\n",
    "    text = ' '.join(df[col])\n",
    "    text = text.split()\n",
    "    freq = pd.Series(text).value_counts()\n",
    "    return freq\n",
    "\n",
    "def remove_common_words(x, freq, n=20):\n",
    "    fn = freq[:n]\n",
    "    x = ' '.join([t for t in x.split() if t not in fn])\n",
    "    return x\n",
    "\n",
    "def remove_rarewords(x, freq, n=20):\n",
    "    fn = freq.tail(n)\n",
    "    x = ' '.join([t for t in x.split() if t not in fn])\n",
    "    return x\n",
    "\n",
    "def remove_dups_char(x):\n",
    "    x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    return x\n",
    "\n",
    "def spelling_correction(x):\n",
    "    x = TextBlob(x).correct()\n",
    "    return x\n",
    "\n",
    "def get_basic_features(df):\n",
    "    if type(df) == pd.core.frame.DataFrame:\n",
    "        df['char_counts'] = df['Tweet'].apply(lambda x: get_charcounts(x))\n",
    "        df['word_counts'] = df['Tweet'].apply(lambda x: get_wordcounts(x))\n",
    "        df['avg_wordlength'] = df['Tweet'].apply(lambda x: get_avg_wordlength(x))\n",
    "        df['stopwords_counts'] = df['Tweet'].apply(lambda x: get_stopwords_counts(x))\n",
    "        df['hashtag_counts'] = df['Tweet'].apply(lambda x: get_hashtag_counts(x))\n",
    "        df['mentions_counts'] = df['Tweet'].apply(lambda x: get_mentions_counts(x))\n",
    "        df['digits_counts'] = df['Tweet'].apply(lambda x: get_digit_counts(x))\n",
    "        df['uppercase_counts'] = df['Tweet'].apply(lambda x: get_uppercase_counts(x))\n",
    "    else:\n",
    "        print('ERROR: This function takes only Pandas DataFrame')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e970b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(x):\n",
    "    x = str(x).lower().replace('\\\\', '').replace('_', ' ').replace('.',' ')\n",
    "    x = cont_exp(x)\n",
    "    x = remove_emails(x)\n",
    "    x = remove_urls(x)\n",
    "    x = remove_html_tags(x)\n",
    "    x = remove_rt(x)\n",
    "    x = remove_accented_chars(x)\n",
    "    x = remove_special_chars(x)\n",
    "    x = remove_dups_char(x)\n",
    "    #x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7b72cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(lambda x: get_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
